/future/u/luisgs/miniconda3/envs/py311_env/lib/python3.11/site-packages/transformers/utils/hub.py:106: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Warning: OPENAI_API_KEY is not set
Starting...
2025-03-07 09:31:48,467 - main - INFO - Start ToT...
Start Argument Parsing...
Namespace(backend='Qwen/Qwen2.5-32B-Instruct', inference_server='local', temperature=0.7, task='game24', task_start_index=901, task_end_index=902, naive_run=False, prompt_sample=None, method_generate='propose', method_evaluate='value', method_select='greedy', n_generate_sample=3, n_evaluate_sample=2, n_select_sample=5)
functools.partial(<function gpt at 0x7fe81e7aa020>, model='Qwen/Qwen2.5-32B-Instruct', temperature=0.7)


LGS: Generation -> Propose
Generating response with vLLM...
INFO 03-07 09:31:53 __init__.py:207] Automatically detected platform cuda.
WARNING 03-07 09:31:55 config.py:2448] Casting torch.bfloat16 to torch.float16.
INFO 03-07 09:34:11 config.py:549] This model supports multiple tasks: {'classify', 'reward', 'embed', 'score', 'generate'}. Defaulting to 'generate'.
INFO 03-07 09:34:11 config.py:1382] Defaulting to use mp for distributed inference
WARNING 03-07 09:34:11 arg_utils.py:1187] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.
INFO 03-07 09:34:11 config.py:1555] Chunked prefill is enabled with max_num_batched_tokens=2048.
WARNING 03-07 09:34:11 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
WARNING 03-07 09:34:11 config.py:685] Async output processing is not supported on the current platform type cuda.
INFO 03-07 09:34:11 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='Qwen/Qwen2-7B', speculative_config=None, tokenizer='Qwen/Qwen2-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2-7B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}, use_cached_outputs=False, 
WARNING 03-07 09:34:12 multiproc_worker_utils.py:300] Reducing Torch parallelism from 255 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 03-07 09:34:12 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=809993)[0;0m INFO 03-07 09:34:12 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=809994)[0;0m INFO 03-07 09:34:12 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=809995)[0;0m INFO 03-07 09:34:12 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
